import os
import logging
import operator
from typing import Annotated, Sequence, TypedDict

from dotenv import load_dotenv

load_dotenv()

from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
import uvicorn

from langchain_core.messages import BaseMessage, HumanMessage
from langchain_community.chat_models import ChatTongyi
from langgraph.graph import StateGraph, START, END
from langgraph.prebuilt import ToolNode
from langgraph.checkpoint.memory import MemorySaver

# å¯¼å…¥å·¥å…·
from tools.rag_tool import search_internal_knowledge
from langchain_community.tools.tavily_search import TavilySearchResults

# ==========================================
# 1. å…¨å±€æ—¥å¿—é…ç½® (Logging Setup)
# ==========================================
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(),  # è¾“å‡ºåˆ°æ§åˆ¶å°
        logging.FileHandler("app.log", encoding='utf-8')  # è¾“å‡ºåˆ°æ–‡ä»¶
    ]
)
logger = logging.getLogger(__name__)

# ==========================================
# 2. FastAPI åˆå§‹åŒ–
# ==========================================
app = FastAPI(title="LangGraph Agent API", version="1.0.0")


class ChatRequest(BaseModel):
    query: str
    thread_id: str = "default_user"


class ChatResponse(BaseModel):
    response: str
    thread_id: str


# ==========================================
# 3. Agent æ„å»º
# ==========================================

# åˆå§‹åŒ– LLM (å›ºå®šä½¿ç”¨é€šä¹‰åƒé—®)
llm = ChatTongyi(model="qwen-turbo", temperature=0)

# åˆå§‹åŒ–å·¥å…· (Tavily + RAG)
tavily_tool = TavilySearchResults(max_results=3)
tools = [search_internal_knowledge, tavily_tool]

# ç»‘å®šå·¥å…·
llm_with_tools = llm.bind_tools(tools)


# å®šä¹‰çŠ¶æ€
class AgentState(TypedDict):
    messages: Annotated[Sequence[BaseMessage], operator.add]


# èŠ‚ç‚¹é€»è¾‘
def call_model(state: AgentState):
    messages = state["messages"]
    # è®°å½•æ—¥å¿—
    logger.info(f"ğŸ¤– [Agent] æ­£åœ¨è°ƒç”¨ LLM (Qwen-Turbo)...")
    try:
        response = llm_with_tools.invoke(messages)
        return {"messages": [response]}
    except Exception as e:
        logger.error(f"[Agent] LLM è°ƒç”¨å¤±è´¥: {e}", exc_info=True)
        raise e


def should_continue(state: AgentState):
    messages = state["messages"]
    last_message = messages[-1]

    if last_message.tool_calls:
        logger.info(f"[Agent] å†³ç­–: è°ƒç”¨å·¥å…· ({len(last_message.tool_calls)} ä¸ª)")
        return "tools"

    logger.info("[Agent] å†³ç­–: ç»“æŸå¯¹è¯ï¼Œç”Ÿæˆå›å¤")
    return END


# æ„å»ºå›¾
workflow = StateGraph(AgentState)
workflow.add_node("agent", call_model)
workflow.add_node("tools", ToolNode(tools))

workflow.add_edge(START, "agent")
workflow.add_conditional_edges("agent", should_continue)
workflow.add_edge("tools", "agent")

checkpointer = MemorySaver()
agent_app = workflow.compile(checkpointer=checkpointer)


# ==========================================
# 4. API è·¯ç”±
# ==========================================

@app.on_event("startup")
async def startup_event():
    logger.info("ç³»ç»Ÿå¯åŠ¨å®Œæˆï¼Œæ­£åœ¨ç›‘å¬ç«¯å£ 8000...")


@app.get("/health")
async def health_check():
    return {"status": "ok", "service": "langgraph-agent"}


@app.post("/chat", response_model=ChatResponse)
async def chat_endpoint(request: ChatRequest):
    logger.info(f"[API] æ”¶åˆ°è¯·æ±‚ | Thread: {request.thread_id} | Query: {request.query}")

    config = {"configurable": {"thread_id": request.thread_id}}
    inputs = {"messages": [HumanMessage(content=request.query)]}

    try:
        final_state = agent_app.invoke(inputs, config=config)
        last_message = final_state["messages"][-1]

        logger.info(f"[API] è¯·æ±‚å¤„ç†å®Œæˆï¼Œè¿”å› {len(last_message.content)} å­—ç¬¦")

        return ChatResponse(
            response=last_message.content,
            thread_id=request.thread_id
        )
    except Exception as e:
        logger.error(f" [API] å¤„ç†å¼‚å¸¸: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=str(e))


if __name__ == "__main__":
    uvicorn.run(app, host="0.0.0.0", port=8000)